{
  "pba" : {
    "status" : "ACTIVE",
    "serviceInfo" : {
      "id" : "7e05c0a3-3f3d-4400-8e19-941669cea7ff",
      "serviceType" : "PROCESSOR",  
      "technology" : "kafka-connect",
      "version" : "1.0.6"  
    },
    "processorInfo":{
         "id":"",
         "technology":"kafka-connect",
         "version":"1.0.6",
         "description":"Kafka Connect is a framework included in Apache Kafka that integrates Kafka with other systems."
      },
      "extensionPoints":[
         {
            "technology":"hdfs",
            "description":"The HDFS source connector allows you to export data to Kafka topics from HDFS files in a variety of formats.",
            "uri":{
               "protocol":"hdfs://",
               "address":"<namenode:port>",
               "args":[
                  
               ]
            },
            "attributes":[
               {
                  "key":"hdfs.url",
                  "value":"<hdfs-source-address>",
                  "isReadOnly":false,
                  "help":"The uri of hdfs",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"connector.class",
                  "value":"io.confluent.connect.hdfs.HdfsSourceConnector",
                  "isReadOnly":false,
                  "help":"The class used to connect to the HDFS system",
                  "isRequired":false,
                  "type":"string"
               },
               {
                  "key":"tasks.max",
                  "value":"<hdfs-source-tasks-max>",
                  "isReadOnly":false,
                  "help":"The number of tasks to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"topics",
                  "value":"<hdfs-source-topic-name>",
                  "isReadOnly":false,
                  "help":"The topic to be used from ",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"flush.size",
                  "value":"<hdfs-source-flush-size>",
                  "isReadOnly":false,
                  "help":"The size of flush to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"name",
                  "value":"<hdfs-source-name>",
                  "isReadOnly":false,
                  "help":"The name of hdfs source",
                  "isRequired":false,
                  "type":"string"
               }
            ]
         },
         {
            "technology":"jdbc",
            "description":"The JDBC source connector allows you to export data to Kafka topics from any relational database with a JDBC drive",
            "uri":{
               "protocol":"jdbc:",
               "address":"<jdbc:name:port:db>",
               "args":[
                  
               ]
            },
            "attributes":[
               {
                  "key":"connection.url",
                  "value":"<jdbc:name:port:db>",
                  "isReadOnly":false,
                  "help":"Specifies the database to connect to",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"auto.create",
                  "value":"<jdbc-source-auto-create>",
                  "isReadOnly":false,
                  "help":"Allows us to rely on the connector for creating the table",
                  "isRequired":false,
                  "type":"boolean"
               },
               {
                  "key":"topics",
                  "value":"<jdbc-source-topics>",
                  "isReadOnly":false,
                  "help":"The topic to be used from kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"connector.class",
                  "value":"io.confluent.connect.jdbc.JdbcSinkConnector",
                  "isReadOnly":false,
                  "help":"Connector to be used on database",
                  "isRequired":false,
                  "type":"string"
               },
               {
                  "key":"name",
                  "value":"<jdbc-source-name>",
                  "isReadOnly":false,
                  "help":"Name of the connector",
                  "isRequired":false,
                  "type":"string"
               }
            ]
         },
         {
            "technology":"file",
            "description":"The FileSource Connector reads data from a file and sends it to Kafka",
            "uri":{
               "protocol":"file://",
               "address":"<file-source-path>",
               "args":[
                  {
                     "key":"format",
                     "value":[
						[
                        "xml",
						"csv",
                        "json",
						"orc", 
						"parquet"
						]
                     ],
                     "default":"",
                     "type":"array"
                  }
               ]
            },
            "attributes":[
               {
                  "key":"connector.class",
                  "value":"FileStreamSource",
                  "isReadOnly":false,
                  "help":"The class used to connect to the file",
                  "isRequired":false,
                  "type":"string"
               },
               {
                  "key":"tasks.max",
                  "value":"<file-source-tasks-max>",
                  "isReadOnly":false,
                  "help":"The number of tasks to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"topic",
                  "value":"<file-source-topic-name>",
                  "isReadOnly":false,
                  "help":"The topic to be used",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"file",
                  "value":"<file-source-path>",
                  "isReadOnly":false,
                  "help":"The file to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"name",
                  "value":"<file-source-name>",
                  "isReadOnly":false,
                  "help":"The name of the file stream",
                  "isRequired":false,
                  "type":"string"
               }
            ]
         },
         {
            "technology":"kafka-connect-kafka",
            "description":"The Kafka data source for kafka-connect that allows the application to read streams of data from topics in the Kafka cluster.",
            "uri":{
               "protocol":"kafka://",
               "address":"<Topic Name>",
			   "help":"Kafka url in the format kafka://address:port",
               "args":[
                  {
                     "key":"schema",
                     "value":"<EventTypeSchema>",
                     "type":"string"
                  },
                  {
                     "key":"format",
                     "value":[
						[
                        "String",
                        "Avro",
                        "Binary",
                        "Google Protocol Buffer"
						]
                     ],
                     "type":"array"
                  }
               ]
            },
            "attributes":[
               {
                  "key":"kafka.mode",
                  "value":"standalone|distributed",
                  "regex":".*",
                  "isReadOnly":false,
                  "help":"If the kafka should be used in standalone mode or distributed one.",
                  "isRequired":true,
                  "type":"string"
               },			
               {
                  "key":"bootstrap.servers",
                  "value":"localhost:9092",
                  "regex":".*",
                  "isReadOnly":false,
                  "help":"A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. The client will make use of all servers irrespective of which servers are specified here for bootstrapping—this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form host1:port1,host2:port2,...",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"key.converter.schemas.enable",
                  "value":"true",
                  "isReadOnly":false,
                  "help":"Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply it to.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"value.converter.schemas.enable",
                  "value":"true",
                  "isReadOnly":false,
                  "help":"Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply it to.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"key.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"Default key converter for Kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"value.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"Default Value converter for Kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"internal.key.converter.schemas.enable",
                  "value":"false",
                  "isReadOnly":false,
                  "help":"The internal converter used for offsets, config, and status data is configurable and must be specified, but most users will always want to use the built-in default.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"internal.value.converter.schemas.enable",
                  "value":"false",
                  "isReadOnly":false,
                  "help":"The internal converter used for offsets, config, and status data is configurable and must be specified, but most users will always want to use the built-in default.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"internal.key.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"The internal key converter used for offsets and config data. Offset and config data are never visible outside of Kafka Connect in this format.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"internal.value.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"The internal value converter used for offsets and config data.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.storage.file.filename",
                  "value":"/tmp/connect.offsets",
                  "isReadOnly":false,
                  "help":"Path where the offset will be stored.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.flush.interval.ms",
                  "value":"10000",
                  "isReadOnly":false,
                  "help":"Flush mode for offset. Lower values are useful for testing/debugging.",
                  "isRequired":true,
                  "type":"integer"
               },
               {
                  "key":"group.id",
                  "value":"<source-group_id>",
                  "isReadOnly":false,
                  "help":"Unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.storage.topic",
                  "value":"<source-offset_storage_topic>",
                  "isReadOnly":false,
                  "help":"Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.storage.replication.factor",
                  "value":"1",
                  "isReadOnly":false,
                  "help":"Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.",
                  "isRequired":true,
                  "type":"integer"
               },
               {
                  "key":"config.storage.topic",
                  "value":"<source-config_storage_topic>",
                  "isReadOnly":false,
                  "help":"Topic to use for storing connector and task configurations.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"config.storage.replication.factor",
                  "value":"1",
                  "isReadOnly":false,
                  "help":"Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.",
                  "isRequired":true,
                  "type":"integer"
               },
               {
                  "key":"event.type",
                  "value":"<source.event_type>",
                  "isReadOnly":false,
                  "help":"Event type that will be triggered.",
                  "isRequired":true,
                  "type":"string"
               }
            ]
         }
      ],
      "integrationPoints":[
         {
            "technology":"hdfs",
            "description":"The HDFS sink connector allows you to export data from Kafka topics to HDFS files in a variety of formats.",
            "uri":{
               "protocol":"hdfs://",
               "address":"<namenode:port>",
               "args":[
                  
               ]
            },
            "attributes":[
               {
                  "key":"hdfs.url",
                  "value":"<hdfs-sink-address>",
                  "isReadOnly":false,
                  "help":"The uri of hdfs",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"connector.class",
                  "value":"io.confluent.connect.hdfs.HdfsSinkConnector",
                  "isReadOnly":false,
                  "help":"The class used to connect to the HDFS system",
                  "isRequired":false,
                  "type":"string"
               },
               {
                  "key":"tasks.max",
                  "value":"<hdfs-sink-tasks-max>",
                  "isReadOnly":false,
                  "help":"The number of tasks to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"topics",
                  "value":"<hdfs-sink-topic-name>",
                  "isReadOnly":false,
                  "help":"The topic to be used from kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"flush.size",
                  "value":"<hdfs-sink-flush-size>",
                  "isReadOnly":false,
                  "help":"The size of flush to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"name",
                  "value":"<hdfs-sink-name>",
                  "isReadOnly":false,
                  "help":"The name of the hdsf sink",
                  "isRequired":false,
                  "type":"string"
               }
            ]
         },
         {
            "technology":"jdbc",
            "description":"The JDBC sink connector allows you to export data from Kafka topics to any relational database with a JDBC drive",
            "uri":{
               "protocol":"jdbc:",
               "address":"<jdbc:name:port:db>",
               "args":[
                  
               ]
            },
            "attributes":[
               {
                  "key":"connection.url",
                  "value":"<jdbc:name:port:db>",
                  "isReadOnly":false,
                  "help":"Specifies the database to connect to",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"auto.create",
                  "value":"<jdbc-sink-auto-create>",
                  "isReadOnly":false,
                  "help":"Allows us to rely on the connector for creating the table",
                  "isRequired":false,
                  "type":"boolean"
               },
               {
                  "key":"topics",
                  "value":"<jdbc-sink-topics>",
                  "isReadOnly":false,
                  "help":"The topic to be used from kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"connector.class",
                  "value":"io.confluent.connect.jdbc.JdbcSinkConnector",
                  "isReadOnly":false,
                  "help":"Connector to be used on database",
                  "isRequired":false,
                  "type":"string"
               },
               {
                  "key":"name",
                  "value":"<jdbc-sink-name>",
                  "isReadOnly":false,
                  "help":"Name of the connector",
                  "isRequired":false,
                  "type":"string"
               }
            ]
         },
         {
            "technology":"file",
            "description":"The FileSink Connector reads data from kafka and sends it to file",
            "uri":{
               "protocol":"file://",
               "address":"<file-sink-path>",
               "args":[
                  {
                     "key":"format",
                     "value":[
						[
                        "xml",
						"csv",
                        "json",
						"orc", 
						"parquet"
						]
                     ],
                     "default":"",
                     "type":"array"
                  }
               ]
            },
            "attributes":[
               {
                  "key":"connector.class",
                  "value":"FileStreamSink",
                  "isReadOnly":false,
                  "help":"The class used to connect to the file",
                  "isRequired":false,
                  "type":"string"
               },
               {
                  "key":"tasks.max",
                  "value":"<file-sink-tasks-max>",
                  "isReadOnly":false,
                  "help":"The number of tasks to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"topic",
                  "value":"<file-sink-topic-name>",
                  "isReadOnly":false,
                  "help":"The topic to be used",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"file",
                  "value":"<file-sink-path>",
                  "isReadOnly":false,
                  "help":"The file to be used",
                  "isRequired":false,
                  "type":"integer"
               },
               {
                  "key":"name",
                  "value":"<file-sink-name>",
                  "isReadOnly":false,
                  "help":"The name of the file stream",
                  "isRequired":false,
                  "type":"string"
               }
            ]
         },
         {
            "technology":"kafka-connect-kafka",
            "description":"The Kafka data sink for kafka-connect that allows the application to write streams of data from topics in the Kafka cluster.",
            "uri":{
               "protocol":"kafka://",
               "address":"<Topic Name>",
			   "help":"Kafka url in the format kafka://address:port",
               "args":[
                  {
                     "key":"schema",
                     "value":"<EventTypeSchema>",
                     "type":"string"
                  },
                  {
                     "key":"format",
                     "value":[
						[
                        "String",
                        "Avro",
                        "Binary",
                        "Google Protocol Buffer"
						]
                     ],
                     "type":"array"
                  }
               ]
            },
            "attributes":[
               {
                  "key":"kafka.mode",
                  "value":"standalone|distributed",
                  "regex":".*",
                  "isReadOnly":false,
                  "help":"If the kafka should be used in standalone mode or distributed one.",
                  "isRequired":true,
                  "type":"string"
               },			
               {
                  "key":"bootstrap.servers",
                  "value":"localhost:9092",
                  "regex":".*",
                  "isReadOnly":false,
                  "help":"A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. The client will make use of all servers irrespective of which servers are specified here for bootstrapping—this list only impacts the initial hosts used to discover the full set of servers. This list should be in the form host1:port1,host2:port2,...",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"key.converter.schemas.enable",
                  "value":"true",
                  "isReadOnly":false,
                  "help":"Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply it to.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"value.converter.schemas.enable",
                  "value":"true",
                  "isReadOnly":false,
                  "help":"Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply it to.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"key.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"Default key converter for Kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"value.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"Default Value converter for Kafka",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"internal.key.converter.schemas.enable",
                  "value":"false",
                  "isReadOnly":false,
                  "help":"The internal converter used for offsets, config, and status data is configurable and must be specified, but most users will always want to use the built-in default.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"internal.value.converter.schemas.enable",
                  "value":"false",
                  "isReadOnly":false,
                  "help":"The internal converter used for offsets, config, and status data is configurable and must be specified, but most users will always want to use the built-in default.",
                  "isRequired":true,
                  "type":"boolean"
               },
               {
                  "key":"internal.key.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"The internal key converter used for offsets and config data. Offset and config data are never visible outside of Kafka Connect in this format.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"internal.value.converter",
                  "value":"org.apache.kafka.connect.json.JsonConverter",
                  "isReadOnly":false,
                  "help":"The internal value converter used for offsets and config data.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.storage.file.filename",
                  "value":"/tmp/connect.offsets",
                  "isReadOnly":false,
                  "help":"Path where the offset will be stored.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.flush.interval.ms",
                  "value":"10000",
                  "isReadOnly":false,
                  "help":"Flush mode for offset. Lower values are useful for testing/debugging.",
                  "isRequired":true,
                  "type":"integer"
               },
               {
                  "key":"group.id",
                  "value":"<sink-group_id>",
                  "isReadOnly":false,
                  "help":"Unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.storage.topic",
                  "value":"<sink-offset_storage_topic>",
                  "isReadOnly":false,
                  "help":"Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"offset.storage.replication.factor",
                  "value":"1",
                  "isReadOnly":false,
                  "help":"Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.",
                  "isRequired":true,
                  "type":"integer"
               },
               {
                  "key":"config.storage.topic",
                  "value":"<sink-config_storage_topic>",
                  "isReadOnly":false,
                  "help":"Topic to use for storing connector and task configurations.",
                  "isRequired":true,
                  "type":"string"
               },
               {
                  "key":"config.storage.replication.factor",
                  "value":"1",
                  "isReadOnly":false,
                  "help":"Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.",
                  "isRequired":true,
                  "type":"integer"
               },
               {
                  "key":"event.type",
                  "value":"<sink.event_type>",
                  "isReadOnly":false,
                  "help":"Event type that will be triggered.",
                  "isRequired":true,
                  "type":"string"
               }
            ]
         }
      ],   
    "buildInfo" : {
	  "container" : {
       "docker" : {
          "name" : "kafka-connect:latest",
          "repoBaseUrl" : "armdocker.rnd.ericsson.se",
          "repoPath" : "aia/tooling/data-tools/kafka-connect",
          "imagePath" : "latest",
          "network" : "HOST",
          "portList" : [ ],
          "mountPaths" : [ ],
          "mountName" : [ ],
          "forcePullImage" : true,
          "privileged" : true
        }
      },	
      "dependencies" : ["service:io:kafka:0.10.2.1"]
    },        
"deploymentInfo" : {    
      "servicePorts" : [ ],
      "maturity" : 0,
      "stagingStatus" : true,
      "inProduction" : true,
      "deploymentScope" : "private|public",
      "noOfInstances" : "1",
      "noOfCpuPerInstance" : "1",
      "memorySize" : "<InGB>",
      "envArgs" : [ ],
      "appArgs" : [ ],
      "attributes" : [ ]
    }
  }
}